{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset Creation\n",
    "\n",
    "For part of this project, we will develop our own custom testing dataset for twitter sentiment analysis. This dataset will be roughly 500 entries long and will be hand labelled with the following classes: Positive, Neutral, Negative. This notebook will walk the reader through the steps taken to create the test dataset.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective is to create a high-quality and original dataset for sentiment analysis sourced from the statuses table of the Twitter PostgreSQL Database. We chose to use the PostgreSQl Database instead of the MongoDB Database because the RDBMS table-based system of PostgreSQL is formatted much better to use with AI/ML libraries than the NoSQL JSON format of MongoDB. This is because we can directly query the data from a PostgreSQL table into a Pandas Dataframe, which is the format all of the python AI libraries take. As mentioned before, the dataset will consist of around 500 unique tweets taken from the twitter.statuses table. Each row entry will consist of the full tweet text and a multiclass label (Positive, Neutral, Negative). The dataset size being at 500 is only because this is the minimum length I feel would still produce meaningful results. Larger datasets are always beigger, but since we are hand-labelling the data, we are limited on dataset size. \n",
    "\n",
    "## Approach\n",
    "\n",
    "1. Connect to Twitter PostgreSql database\n",
    "2. Load 500 entries into a Pandas Dataframe.\n",
    "3. Reformat data.\n",
    "4. Export data to a CSV with placeholder labels.\n",
    "5. Go through dataset by hand and label every entry.\n",
    "6. Display resulting dataset and show some distributions, etc. \n",
    "\n",
    "If you are curious about the actualy sentiment analysis, see the file \"SentimentAnalysis.ipynb\". I will start by importing the necessary libraries and connecting to the Twitter comp_dbms PostgreSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.\n",
    "Now I will connect to the database and display some basic information about the table twitter.statuses. This is where we will be getting the data to construct the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = psycopg2.connect(host='3.230.203.12',\n",
    "                             user='compdb',\n",
    "                             port=5438,\n",
    "                             database='twitter',\n",
    "                             password='compdbs_postgres')\n",
    "connection.set_session(readonly=True, autocommit=True)\n",
    "\n",
    "#From our connection we need a cursor, which acts as our interface into the database\n",
    "cur = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. \n",
    "\n",
    "Now I will load 500 entries from the twitter.statuses table into a Pandas Dataframe. I built this specific query for 3 reasons. First, I use distinct(status_id) to make sure there are not duplicate statuses. This is important because repeated entries can lead to slight biases and inconsistencies when testing, which is especially true for a smaller dataset. Second, I filter on language to only include english entries. Sentiment analysis typically works better when limited to one language, and English seemed like the obvious choice since it is my native language. Lastly, I use a subquery to filter out all the statuses containing links. I noticed while building this dataset that some statuses contain links to pictures or wesites, and some tweets contain JUST these links. Having links in the tweets will definitely throw off the accuracy of the sentiment analysis, so I left them out. This may lead to slight inconsistencies in the data (EX: What if one user has only tweeted images? Now they are not represented by the data at all), but I believe these inconsistencies are so minor they are negligible. I have printed the head (first 5 entries) for visualization of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000479232273743872</td>\n",
       "      <td>I don't know about you but I think Alden Ehren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000978713410859008</td>\n",
       "      <td>RT @mitmul: The PR for DLPack support in CuPy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001167950232080385</td>\n",
       "      <td>RT @bonnienorman: @BretStephensNYT Oh ffs. Suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001533153721413633</td>\n",
       "      <td>RT @egrefen: Super cool @PyTorch reimplementat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001549898247000064</td>\n",
       "      <td>RT @SpaceX: Now targeting June 1 launch of SES...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                                                  1\n",
       "0  1000479232273743872  I don't know about you but I think Alden Ehren...\n",
       "1  1000978713410859008  RT @mitmul: The PR for DLPack support in CuPy ...\n",
       "2  1001167950232080385  RT @bonnienorman: @BretStephensNYT Oh ffs. Suc...\n",
       "3  1001533153721413633  RT @egrefen: Super cool @PyTorch reimplementat...\n",
       "4  1001549898247000064  RT @SpaceX: Now targeting June 1 launch of SES..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = cur.execute(\"\"\"Select distinct(status_id), text \n",
    "                     from twitter.statuses s \n",
    "                     where s.lang = 'en' and status_id not in (\n",
    "                         select status_id from twitter.statuses s2\n",
    "                         where text like '%https://%') \n",
    "                     limit 1000\"\"\"\n",
    "                 )\n",
    "users = cur.fetchall()\n",
    "df = pd.DataFrame.from_dict(users)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have also displayed some basic data about the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1000 non-null   object\n",
      " 1   1       1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Can see the following information: there are 9 un-named columns of various data types in this dataframe. We are only concerned with the column representing the text of each tweet, which we can figure out is column index 6 from the .head() printout. In the next step we will process this dataframe.\n",
    "\n",
    "\n",
    "## Step 3. \n",
    "\n",
    "To reformat the data, we will drop the columns we do not need, label the remaining column, and add a new labelled column for sentiment with all 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't know about you but I think Alden Ehren...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @mitmul: The PR for DLPack support in CuPy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @bonnienorman: @BretStephensNYT Oh ffs. Suc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @egrefen: Super cool @PyTorch reimplementat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SpaceX: Now targeting June 1 launch of SES...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  I don't know about you but I think Alden Ehren...       0\n",
       "1  RT @mitmul: The PR for DLPack support in CuPy ...       0\n",
       "2  RT @bonnienorman: @BretStephensNYT Oh ffs. Suc...       0\n",
       "3  RT @egrefen: Super cool @PyTorch reimplementat...       0\n",
       "4  RT @SpaceX: Now targeting June 1 launch of SES...       0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all columns besides the 6th indexed\n",
    "df = df.iloc[:, [1]]\n",
    "# Add title to twitter text column\n",
    "df = df.rename(columns={df.columns[0]: 'text'})\n",
    "# Add a new column with zeros for sentiment target\n",
    "df['target'] = 0\n",
    "\n",
    "# Display info\n",
    "print(f\"Num rows: {df.shape[0]}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above printout, we are now left with a Dataframe containing the information we want.\n",
    "\n",
    "\n",
    "### Step 4. \n",
    "\n",
    "Now I will take the appropriate steps to export this dataframe to a CSV file named CustomTwitterSentimentiments.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to a CSV file\n",
    "df.to_csv('Data/CustomTwitterSentiments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 5 & 6. \n",
    "\n",
    "Now I will load back in the labelled CSV file in for some basic analysis. The data has been labelled with the following: -1 = negative, 0 = neutral, 1 = positive. I labelled the dataset this way to keep it identical in format to the Twitter_Data dataset. This is a dataset with 1.6 million tweets which I will use for training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
